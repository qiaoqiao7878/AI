{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "\n",
    "This notebook serves as supporting material for **Lecture 13 - Learning** from the lecture Grundlagen der KÃ¼nstlichen Intelligenz (IN2062). This notebook is absorbed from [learning.ipynb](https://github.com/aimacode/aima-python/blob/master/learning.ipynb) and uses implementations from [learning.py](https://github.com/aimacode/aima-python/blob/master/learning.py) of the AIMA repository from the book *Artificial Intelligence: A Modern Approach*. Let's start by importing everything from the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from learning import *\n",
    "from notebook import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "* Introduction\n",
    "\n",
    "* Decision Tree Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "You can find the code for the dataset here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "psource(DataSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Attributes\n",
    "\n",
    "* **examples**: Holds the items of the dataset. Each item is a list of values.\n",
    "\n",
    "* **attrs**: The indexes of the features (by default in the range of [0,f), where *f* is the number of features). For example, `item[i]` returns the feature at index *i* of *item*.\n",
    "\n",
    "* **attrnames**: An optional list with attribute names. For example, `item[s]`, where *s* is a feature name, returns the feature of name *s* in *item*.\n",
    "\n",
    "* **target**: The attribute a learning algorithm will try to predict. By default the last attribute.\n",
    "\n",
    "* **inputs**: This is the list of attributes without the target.\n",
    "\n",
    "* **values**: A list of lists which holds the set of possible values for the corresponding attribute/feature. If initially `None`, it gets computed (by the function `setproblem`) from the examples.\n",
    "\n",
    "* **distance**: The distance function used in the learner to calculate the distance between two items. By default `mean_boolean_error`.\n",
    "\n",
    "* **name**: Name of the dataset.\n",
    "\n",
    "* **source**: The source of the dataset (url or other). Not used in the code.\n",
    "\n",
    "* **exclude**: A list of indexes to exclude from `inputs`. The list can include either attribute indexes (attrs) or names (attrnames)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Helper Functions\n",
    "\n",
    "These functions help modify a `DataSet` object to your needs.\n",
    "\n",
    "* **sanitize**: Takes as input an example and returns it with non-input (target) attributes replaced by `None`. Useful for testing. Keep in mind that the example given is not itself sanitized, but instead a sanitized copy is returned.\n",
    "\n",
    "* **classes_to_numbers**: Maps the class names of a dataset to numbers. If the class names are not given, they are computed from the dataset values. Useful for classifiers that return a numerical value instead of a string.\n",
    "\n",
    "* **remove_examples**: Removes examples containing a given value. Useful for removing examples with missing values, or for removing classes (needed for binary classifiers).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE LEARNER\n",
    "\n",
    "### Overview\n",
    "\n",
    "#### Decision Trees\n",
    "A decision tree is a flowchart that uses a tree of decisions and their possible consequences for classification. At each non-leaf node of the tree an attribute of the input is tested, based on which corresponding branch leading to a child-node is selected. At the leaf node the input is classified based on the class label of this leaf node. The paths from root to leaves represent classification rules based on which leaf nodes are assigned class labels.\n",
    "<img src=\"decision_tree_learner_restaurant.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "#### Decision Tree Learning\n",
    "Decision tree learning is the construction of a decision tree from class-labeled training data. The data is expected to be a tuple in which each record of the tuple is an attribute used for classification. The decision tree is built top-down, by choosing a variable at each step that best splits the set of items. There are different metrics for measuring the \"best split\". These generally measure the homogeneity of the target variable within the subsets.\n",
    "\n",
    "#### Gini Impurity\n",
    "Gini impurity of a set is the probability of a randomly chosen element to be incorrectly labeled if it was randomly labeled according to the distribution of labels in the set.\n",
    "\n",
    "$$I_G(p) = \\sum{p_i(1 - p_i)} = 1 - \\sum{p_i^2}$$\n",
    "\n",
    "We select a split which minimizes the Gini impurity in child nodes.\n",
    "\n",
    "#### Information Gain\n",
    "Information gain is based on the concept of entropy from information theory. Entropy is defined as:\n",
    "\n",
    "$$B(p) = -\\sum{p_i \\log_2{p_i}}$$\n",
    "\n",
    "Information Gain is difference between entropy of the parent and weighted sum of entropy of children. The feature used for splitting is the one which provides the most information gain.\n",
    "\n",
    "#### Pseudocode\n",
    "\n",
    "You can view the pseudocode by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pseudocode(\"Decision Tree Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "The nodes of the tree constructed by our learning algorithm are stored using either `DecisionFork` or `DecisionLeaf` based on whether they are a parent node or a leaf node respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psource(DecisionFork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DecisionFork` holds the attribute, which is tested at that node, and a dict of branches. The branches store the child nodes, one for each of the attribute's values. Calling an object of this class as a function with input tuple as an argument returns the next node in the classification path based on the result of the attribute test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "psource(DecisionLeaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leaf node stores the class label in `result`. All input tuples' classification paths end on a `DecisionLeaf` whose `result` attribute decide their class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "psource(DecisionTreeLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of `DecisionTreeLearner` provided in [learning.py](https://github.com/aimacode/aima-python/blob/master/learning.py) uses information gain as the metric for selecting which attribute to test for splitting. The function builds the tree top-down in a recursive manner. Based on the input it makes one of the four choices:\n",
    "<ol>\n",
    "<li>If the input at the current step has no training data we return the mode of classes of input data received in the parent step (previous level of recursion).</li>\n",
    "<li>If all values in training data belong to the same class it returns a `DecisionLeaf` whose class label is the class which all the data belongs to.</li>\n",
    "<li>If the data has no attributes that can be tested we return the class with highest plurality value in the training data.</li>\n",
    "<li>We choose the attribute which gives the highest amount of entropy gain and return a `DecisionFork` which splits based on this attribute. Each branch recursively calls `decision_tree_learning` to construct the sub-tree.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "We will now use the Decision Tree Learner to learn a decision tree for the example from **lecture 13, slide 13**.\n",
    "First, create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attrnames = ['Alt', 'Bar', 'Fri', 'Hun', 'Pat', 'Price', 'Rain', 'Res', 'Type', 'Est', 'WillWait']\n",
    "examples = [\n",
    "            ['T', 'F', 'F', 'T', 'Some', '$$$', 'F', 'T', 'French',  '0-10',  'T'],\n",
    "            ['T', 'F', 'F', 'T', 'Full', '$',   'F', 'F', 'Thai',    '30-60', 'F'], \n",
    "            ['F', 'T', 'F', 'F', 'Some', '$',   'F', 'F', 'Burger',  '0-10',  'T'], \n",
    "            ['T', 'F', 'T', 'T', 'Full', '$',   'F', 'F', 'Thai',    '10-30', 'T'], \n",
    "            ['T', 'F', 'T', 'F', 'Full', '$$$', 'F', 'T', 'French',  '>60',   'F'], \n",
    "            ['F', 'T', 'F', 'T', 'Some', '$$',  'T', 'T', 'Italian', '0-10',  'T'], \n",
    "            ['F', 'T', 'F', 'F', 'None', '$',   'T', 'F', 'Burger',  '0-10',  'F'], \n",
    "            ['F', 'F', 'F', 'T', 'Some', '$$',  'T', 'T', 'Thai',    '0-10',  'T'], \n",
    "            ['F', 'T', 'T', 'F', 'Full', '$',   'T', 'F', 'Burger',  '>60',   'F'], \n",
    "            ['T', 'T', 'T', 'T', 'Full', '$$$', 'F', 'T', 'Italian', '10-30', 'F'], \n",
    "            ['F', 'F', 'F', 'F', 'None', '$',   'F', 'F', 'Thai',    '0-10',  'F'], \n",
    "            ['T', 'T', 'T', 'T', 'Full', '$',   'F', 'F', 'Burger',  '30-60', 'T']\n",
    "           ]\n",
    "\n",
    "lecture_dataset = DataSet(examples=examples, attrnames=attrnames, target='WillWait')\n",
    "lecture_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTL = DecisionTreeLearner(lecture_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the tree properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTL.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Note</h3>\n",
    "    <p>The calculated decision tree could be different from the one showed in the lecture slide 19. The reason is that after spliting the samples with Patrons, the information gain for the five attributes: Hun, Price, Res, Type, and Est are equal. Therefore, the algorithm randomly choose one of the five attribute and split the samples further. We illustrate this by calculating the information gain values in the following</p> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"decision_tree_restaurant_result.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We adapt the functions from learning.py first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by(attr, examples, values):\n",
    "        \"\"\"Return a list of (val, examples) pairs for each val of attr.\"\"\"\n",
    "        return [(v, [e for e in examples if e[attr] == v])\n",
    "                for v in values[attr]]\n",
    "\n",
    "def count(attr, val, examples):\n",
    "    \"\"\"Count the number of examples that have example[attr] = val.\"\"\"\n",
    "    return sum(e[attr] == val for e in examples)\n",
    "\n",
    "def information_gain(attr, examples, values, target):\n",
    "    \"\"\"Return the expected reduction in entropy from splitting by attr.\"\"\"\n",
    "    def I(examples):\n",
    "        return information_content([count(target, v, examples)\n",
    "                                    for v in values[target]])\n",
    "    N = len(examples)\n",
    "    remainder = sum((len(examples_i)/N) * I(examples_i)\n",
    "                    for (v, examples_i) in split_by(attr, examples, values))\n",
    "    return I(examples) - remainder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get three child datasets after spliting the samples with attribute Patrons. All the samples of the child dataset for Pat=\"None\" and for Pat=\"Some\" have the same value for the target. So we only need to split the tree further for Pat=\"Full\". We define the child dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrnames = ['Alt', 'Bar', 'Fri', 'Hun', 'Price', 'Rain', 'Res', 'Type', 'Est', 'WillWait']\n",
    "examples = [\n",
    "            ['T', 'F', 'F', 'T', '$',   'F', 'F', 'Thai',    '30-60', 'F'], \n",
    "            ['T', 'F', 'T', 'T', '$',   'F', 'F', 'Thai',    '10-30', 'T'], \n",
    "            ['T', 'F', 'T', 'F', '$$$', 'F', 'T', 'French',  '>60',   'F'],\n",
    "            ['F', 'T', 'T', 'F', '$',   'T', 'F', 'Burger',  '>60',   'F'], \n",
    "            ['T', 'T', 'T', 'T', '$$$', 'F', 'T', 'Italian', '10-30', 'F'],\n",
    "            ['T', 'T', 'T', 'T', '$',   'F', 'F', 'Burger',  '30-60', 'T']\n",
    "           ]\n",
    "child_dataset = DataSet(examples=examples, attrnames=attrnames, target='WillWait')\n",
    "child_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in child_dataset.inputs:\n",
    "    info_gain = information_gain(attr, child_dataset.examples, \n",
    "                                 child_dataset.values, child_dataset.target)\n",
    "    print(\"Information gain of attribute {} is {}\".format(child_dataset.attrnames[attr], info_gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the information gain of Hun, Price, Res, Type, and Est are all 0.2516. So it does not matter along which of the five attributes to split further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the learned decision tree to classify a sample with \n",
    "    'Alt'= 'F', 'Bar'='F', 'Fri'='F', 'Hun='F', 'Pat'='None', 'Price'='$', 'Rain'='F', 'Res'='F', 'Type'='Thai', 'Est'='0-10'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DTL(['F', 'F', 'F', 'F','None', '$', 'F', 'F', 'Thai', '0-10']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Decision Tree learner classifies the sample as \"F\"."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
