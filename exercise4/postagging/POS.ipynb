{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Problem 2: Part-of-speech tagging using the Viterbi algorithm\n",
    "\n",
    "### Learning outcome\n",
    "In this exercise, you learn how to implement the Viterbi algorithm on part-of-speech (POS) tagging of sentences.\n",
    "\n",
    "### Introduction\n",
    "A short introduction to POS can be found here: https://www.youtube.com/watch?v=mHEKZ8jv2SY\n",
    "\n",
    "In this notebook, first the data constisting of a text and the corresponding grammatical tags of each word is parsed and processed. Then, a Hidden Markov Model (HMM) is implemented: The observed variables are the tokens, which are words or punctuation characters like `the`, `blue` or `.`. The hidden variables are the grammatical tags which take values like `NOUN` or `VERB` or `.` for punctuation characters. Subsequently, the probabilities of the model are learned for the given data. When only the sentences of previously unknown test data are provided, the most likely sequence of tags has to be assigned correctly to each sentence by implementing the Viterbi algorithm is implemented for the HMM.\n",
    "\n",
    "### Passing criteria:\n",
    "- For passing this exercise, the `final_test` in the last cell needs to pass when running the complete notebook. The same test is performed when the notebbok is pushed to Artemis. (Make sure that all the cells run without an error or comment them out otherwise.)\n",
    "\n",
    "- No other libraries than numpy may be used. The `*.pkl` files used for the provided tests must not be used for anything besides these tests.\n",
    "\n",
    "Fill out the TODOs in the stub of the code which is provided here (recommended) or feel free to create your own model or. However, the names of the provided functions and their input/output arguments must not be changed in order to run the final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install numpy  # uncomment this and rerun in case numpy is not installed\n",
    "import time\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the dataset\n",
    "First, the dataset is downloaded. It can either be downloaded from https://raw.githubusercontent.com/davidpitkanen/Part-of-Speech-Tagger/master/brown-universal.txt or by executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset\n"
     ]
    }
   ],
   "source": [
    "filename=\"brown-universal.txt\"\n",
    "\n",
    "try:\n",
    "    with open(filename) as f:\n",
    "        print(\"Dataset already downloaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Downloading dataset\")\n",
    "    url = \"https://raw.githubusercontent.com/davidpitkanen/Part-of-Speech-Tagger/master/brown-universal.txt\"\n",
    "    r = requests.get(url)\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, the data is read and - in a Machine Learning manner - split into training, validation and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_dataset(filename=\"brown-universal.txt\", number_of_sentences_to_take=None):\n",
    "    \"\"\"\n",
    "    Reads the dataset and splits it into training, validation and test data\n",
    "    The dataset is the Brown Corpus (https://en.wikipedia.org/wiki/Brown_Corpus), annotated with\n",
    "    the universal tagset: http://www.nltk.org/nltk_data/\n",
    "    The dataset is taken from:\n",
    "        https://github.com/davidpitkanen/Part-of-Speech-Tagger/blob/master/brown-universal.txt\n",
    "\n",
    "    :param filename: str, the filename of the dataset\n",
    "    :param number_of_sentences_to_take: None or int, if the latter: for faster debugging, only read the\n",
    "        first number_of_sentences_to_take into the dataset\n",
    "    :return: train_sentences, val_sentences, test_sentences, each [[(str,str)]] list of sentences, each \n",
    "        sentence is a list of tuples with the first element being the token, the second element the label\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        current_sentence = []\n",
    "        for line in f:\n",
    "            if line.__contains__(\"b100-\"):  # start of a new sentence\n",
    "                current_sentence = []\n",
    "            elif line == \"\\n\":  # end of a sentence\n",
    "                sentences.append(current_sentence)\n",
    "            else:  # the line contains the next token (i.e. a word ,'.', ',', ...) of \n",
    "                # the current sentence and its label\n",
    "                token, label = line.split()\n",
    "                current_sentence.append((token.lower(), label))\n",
    "    \n",
    "    # reduced numer of sentences for faster debugging:\n",
    "    if number_of_sentences_to_take:\n",
    "        sentences = sentences[:number_of_sentences_to_take]\n",
    "\n",
    "    train_idx = int(np.ceil((len(sentences) * 0.9)))\n",
    "    text_idx = int((np.ceil(len(sentences) * 0.95)))\n",
    "\n",
    "    train_sentences = sentences[:train_idx]\n",
    "    val_sentences = sentences[train_idx:text_idx]\n",
    "    test_sentences = sentences[train_idx:]  # for final testing \n",
    "\n",
    "    return train_sentences, val_sentences, test_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, take a look at the dataset and use only a few sentences for faster debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('mr.', 'NOUN'),\n",
       "  ('podger', 'NOUN'),\n",
       "  ('had', 'VERB'),\n",
       "  ('thanked', 'VERB'),\n",
       "  ('him', 'PRON'),\n",
       "  ('gravely', 'ADV'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('now', 'ADV'),\n",
       "  ('he', 'PRON'),\n",
       "  ('made', 'VERB'),\n",
       "  ('use', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('advice', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('but', 'CONJ'),\n",
       "  ('there', 'PRT'),\n",
       "  ('seemed', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('be', 'VERB'),\n",
       "  ('some', 'DET'),\n",
       "  ('difference', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('opinion', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('to', 'ADP'),\n",
       "  ('how', 'ADV'),\n",
       "  ('far', 'ADV'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('should', 'VERB'),\n",
       "  ('go', 'VERB'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('whose', 'DET'),\n",
       "  ('advice', 'NOUN'),\n",
       "  ('it', 'PRON'),\n",
       "  ('should', 'VERB'),\n",
       "  ('follow', 'VERB'),\n",
       "  ('.', '.')],\n",
       " [('such', 'PRT'),\n",
       "  ('an', 'DET'),\n",
       "  ('instrument', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('expected', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('be', 'VERB'),\n",
       "  ('especially', 'ADV'),\n",
       "  ('useful', 'ADJ'),\n",
       "  ('if', 'ADP'),\n",
       "  ('it', 'PRON'),\n",
       "  ('could', 'VERB'),\n",
       "  ('be', 'VERB'),\n",
       "  ('used', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('measure', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('elasticity', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('heavy', 'ADJ'),\n",
       "  ('pastes', 'NOUN'),\n",
       "  ('such', 'ADJ'),\n",
       "  ('as', 'ADP'),\n",
       "  ('printing', 'VERB'),\n",
       "  ('inks', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('paints', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('adhesives', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('molten', 'ADJ'),\n",
       "  ('plastics', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('bread', 'NOUN'),\n",
       "  ('dough', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('for', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('elasticity', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('related', 'VERB'),\n",
       "  ('to', 'ADP'),\n",
       "  ('those', 'DET'),\n",
       "  ('various', 'ADJ'),\n",
       "  ('properties', 'NOUN'),\n",
       "  ('termed', 'VERB'),\n",
       "  ('``', '.'),\n",
       "  ('length', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  (',', '.'),\n",
       "  ('``', '.'),\n",
       "  ('shortness', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  (',', '.'),\n",
       "  ('``', '.'),\n",
       "  ('spinnability', 'NOUN'),\n",
       "  (\"''\", '.'),\n",
       "  (',', '.'),\n",
       "  ('etc.', 'ADV'),\n",
       "  (',', '.'),\n",
       "  ('which', 'DET'),\n",
       "  ('are', 'VERB'),\n",
       "  ('usually', 'ADV'),\n",
       "  ('judged', 'VERB'),\n",
       "  ('by', 'ADP'),\n",
       "  ('subjective', 'ADJ'),\n",
       "  ('methods', 'NOUN'),\n",
       "  ('at', 'ADP'),\n",
       "  ('present', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('my', 'DET'),\n",
       "  ('future', 'ADJ'),\n",
       "  ('plans', 'NOUN'),\n",
       "  ('are', 'VERB'),\n",
       "  ('to', 'PRT'),\n",
       "  ('become', 'VERB'),\n",
       "  ('a', 'DET'),\n",
       "  ('language', 'NOUN'),\n",
       "  ('teacher', 'NOUN'),\n",
       "  ('.', '.')],\n",
       " [('we', 'PRON'),\n",
       "  ('ran', 'VERB'),\n",
       "  ('east', 'NOUN'),\n",
       "  ('for', 'ADP'),\n",
       "  ('about', 'ADV'),\n",
       "  ('half', 'PRT'),\n",
       "  ('a', 'DET'),\n",
       "  ('mile', 'NOUN'),\n",
       "  ('before', 'ADP'),\n",
       "  ('we', 'PRON'),\n",
       "  ('turned', 'VERB'),\n",
       "  ('back', 'ADV'),\n",
       "  ('to', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('road', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('panting', 'VERB'),\n",
       "  ('from', 'ADP'),\n",
       "  ('the', 'DET'),\n",
       "  ('effort', 'NOUN'),\n",
       "  ('and', 'CONJ'),\n",
       "  ('soaked', 'VERB'),\n",
       "  ('with', 'ADP'),\n",
       "  ('sweat', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences, val_sentences, test_sentences = read_dataset()\n",
    "\n",
    "train_sentences_debug = train_sentences[:5]\n",
    "train_sentences_debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count transistions and emissions\n",
    "In the following, several helper functions with the corresponding tests are defined. The tests load sample inputs and expected outputs from the corresponding pickle files. The tests should pass without an error. In case there are errors, use your IDEs debugger or print statements to figure out why the results are different. (Or ignore them if you think your implementation should work anyway.)\n",
    "\n",
    "TODO: Figure out and implement what the methods should do by looking at their signatures and the PartsOfSpeechTaggingModel class below.\n",
    "Wherever code needs to be inserted, it is marked with a \"TODO\"-comment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_transitions(label_to_idx, sentences):\n",
    "    \"\"\"\n",
    "    Counts the transitions from one label i to another label j\n",
    "\n",
    "    :param label_to_idx: dict {str : int} mapping from labels to ids\n",
    "    :param sentences: [[(str,str)]] list of sentences, each sentence is a list of tuples with the first element\n",
    "        being the token, the second element the label\n",
    "    :return: transition_counts: ndarray, shape = (#labels, #labels)\n",
    "    transition_counts[i,j] : How many times follows idx_to_label[j] after idx_to_label[i] in the dataset?\n",
    "    \"\"\"\n",
    "\n",
    "    transition_counts = np.zeros(shape=(len(label_to_idx), len(label_to_idx)))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        before_label = None\n",
    "        for _, label in sentence:\n",
    "            if before_label:\n",
    "                # TODO: Your code here\n",
    "                #####\n",
    "                transition_counts[label_to_idx[before_label], label_to_idx[label]] =\n",
    "                #####\n",
    "            before_label = label\n",
    "\n",
    "    return transition_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_count_transitions():\n",
    "    label_to_idx, sentences, transition_counts = pickle.load(open(\"count_transitions_test.pkl\", \"rb\"))\n",
    "\n",
    "    assert (count_transitions(label_to_idx, sentences) == transition_counts).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_count_transitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_start_end_labels(sentences, label_to_idx):\n",
    "    \"\"\"\n",
    "    Counts the start and end labels for the sentences\n",
    "\n",
    "    :param sentences: [[(str,str)]] list of sentences, each sentence is a list of tuples with the \n",
    "        first element being the token, the second element the label\n",
    "    :param label_to_idx: dict {str : int} mapping from labels to ids\n",
    "    :return: start_label_count_vector, end_label_count_vector, both ndarray , both: shape = (#labels,)\n",
    "    start_label_count_vector[i]: How many times appeared idx_to_token[i] in the beginning of a sentence \n",
    "        in the dataset?\n",
    "    end_label_count_vector[i]: How many times appeared idx_to_token[i] in the end of a sentence in the dataset?\n",
    "    \"\"\"\n",
    "\n",
    "    start_labels = [sentence[0][1] for sentence in\n",
    "                    sentences]  # labels of the first token of each sentence\n",
    "    #####\n",
    "    # TODO: Your code here\n",
    "    end_labels =  # labels of the last token of each sentence \n",
    "    #####\n",
    "    start_label_count = dict(Counter(start_labels))\n",
    "    end_label_count = dict(Counter(end_labels))\n",
    "\n",
    "    start_label_count_vector = np.zeros(shape=len(label_to_idx))\n",
    "    end_label_count_vector = np.zeros(shape=len(label_to_idx))\n",
    "    for label in label_to_idx.keys():\n",
    "        if label in start_label_count.keys():\n",
    "            #####\n",
    "            # TODO: Your code here            \n",
    "            start_label_count_vector[label_to_idx[label]] \n",
    "            #####\n",
    "        if label in end_label_count.keys():\n",
    "            #####\n",
    "            # TODO: Your code here            \n",
    "            end_label_count_vector[label_to_idx[label]] \n",
    "            #####\n",
    "\n",
    "    return start_label_count_vector, end_label_count_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_count_start_end_labels():\n",
    "    train_sentences, label_to_idx, start_label_count_vector, end_label_count_vector = pickle.load(\n",
    "        open(\"count_start_end_labels_test.pkl\", \"rb\"))\n",
    "\n",
    "    result1, result2 = count_start_end_labels(train_sentences, label_to_idx)\n",
    "\n",
    "    assert (result1 == start_label_count_vector).all()\n",
    "    assert (result2 == end_label_count_vector).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_count_start_end_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def count_emissions(token_list, labels_list, common_tokens, token_to_idx, label_to_idx):\n",
    "    \"\"\"\n",
    "    Counts the emissions of labels for each token\n",
    "\n",
    "    :param token_list:  [str] list of strings, each corresponding to a token\n",
    "    :param labels_list: [str] list of strings, each corresponding to a label of a token\n",
    "    :param common_tokens: [str] list of strings, each corresponding to a known token\n",
    "    :param token_to_idx: dict {str : int} mapping from tokens to ids\n",
    "    :param label_to_idx: dict {str : int} mapping from labels to ids\n",
    "    :return: emission_counts: ndarray, shape = (#tokens (including placeholder), #labels)\n",
    "    emission_counts[i,j] : How many times occur idx_to_token[i] and idx_to_label[j] together in the dataset?\n",
    "\n",
    "    \"\"\"\n",
    "    emission_counts = np.zeros(shape=(len(set(common_tokens)) + 1, len(set(\n",
    "        labels_list))))  # +1 for the placeholder token\n",
    "    combination_counts = Counter(zip(token_list, labels_list))\n",
    "    for token, label in dict(combination_counts).keys():\n",
    "        if token in common_tokens:\n",
    "            # TODO: Your code here\n",
    "            #####\n",
    "            emission_counts[token_to_idx[token], label_to_idx[label]] += \n",
    "            #####\n",
    "        else:\n",
    "            emission_counts[0, label_to_idx[label]] += combination_counts[\n",
    "                (token, label)]  # counts as placeholder token\n",
    "\n",
    "   \n",
    "    return emission_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def test_count_emissions():\n",
    "    labels_list, token_list, common_tokens, token_to_idx, label_to_idx, emission_counts = pickle.load(\n",
    "        open(\"count_emissions_test.pkl\", \"rb\"))\n",
    "    assert (count_emissions(token_list, labels_list, common_tokens, token_to_idx,\n",
    "                            label_to_idx) == emission_counts).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_count_emissions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning parameters of the HMM model\n",
    "Now, the model with the HMM can be defined and its parameters are learned. All the words/tokens which do not appear often in the training data will be mapped to a placeholder token. In this way, the model learns better how to deal with unseen tokens in the validation/test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PartsOfSpeechTaggingModel:\n",
    "    \"\"\"\n",
    "    This Class represents a model for performing part-of-speech tagging on sentences\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_sentences, min_token_count_required=3, epsilon=1.e-5,\n",
    "                 placeholder_token=\"UNKNOWN_PLACEHOLDER_TOKEN\"):\n",
    "        \"\"\"\n",
    "        In this init method for the PartsOfSpeechTaggingModel, the dataset is read, preprocessed\n",
    "        and the 'training' is performed by creating a hidden Markov model\n",
    "\n",
    "        :param train_sentences: [[(str,str)]] list of sentences, each sentence is a list of tuples with \n",
    "            the first element being the token, the second element the label; the data to train the model with\n",
    "        :param min_token_count_required: int, the minimum number of occurrences of a token such that it \n",
    "            will be considered as 'known' and not mapped to the placeholder token\n",
    "        :param epsilon: float, a small value for numerical reasons and as a bias towards rare, unseen \n",
    "            transitions\n",
    "        :param placeholder_token: str, a string which will be inserted for every unknown token\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        self.placeholder_token = placeholder_token\n",
    "        labels_list = [label for sentence in train_sentences for _, label in sentence]\n",
    "        token_list = [token for sentence in train_sentences for token, _ in sentence]\n",
    "        unique_labels_ordered = sorted(list(set(labels_list)))\n",
    "        self.label_to_idx = {label: i for i, label in enumerate(unique_labels_ordered)}\n",
    "        # TODO: Your code here\n",
    "        #####\n",
    "        self.idx_to_label: dict =\n",
    "        #####\n",
    "        token_counts = Counter(token_list)\n",
    "        self.common_tokens = sorted(\n",
    "            list(set([token for token in set(token_list) if token_counts[token] >= min_token_count_required])))\n",
    "        self.token_to_idx = {token: i + 1 for i, token in\n",
    "                             enumerate(self.common_tokens)}  # idx 0 is reserved for placeholder token\n",
    "        self.token_to_idx[placeholder_token] = 0\n",
    "        # TODO: Your code here\n",
    "        #####\n",
    "        self.idx_to_token: dict =\n",
    "        self.idx_to_token[0] =\n",
    "        #####\n",
    "\n",
    "        emission_count_matrix = count_emissions(token_list, labels_list, self.common_tokens, self.token_to_idx,\n",
    "                                                self.label_to_idx)\n",
    "\n",
    "        start_label_count_vector, end_label_count_vector = count_start_end_labels(train_sentences,\n",
    "                                                                                  self.label_to_idx)\n",
    "        transition_count_matrix = count_transitions(self.label_to_idx, train_sentences)\n",
    "\n",
    "        # for numerical reasons and bias towards rare/unseen transitions\n",
    "        start_label_count_vector += epsilon\n",
    "        end_label_count_vector += epsilon\n",
    "        emission_count_matrix += epsilon\n",
    "        transition_count_matrix += epsilon\n",
    "        \n",
    "        #####\n",
    "        # TODO: Your code here        \n",
    "        # start_label_probabilities[i] : Probability that the first label of a sentence is idx_to_label[i]\n",
    "        self.start_label_probabilities =\n",
    "        # analogously for last label:\n",
    "        self.end_label_probabilities =        \n",
    "        #####\n",
    "        \n",
    "        # emission_probabilities[i,j] : Probability that idx_to_token[i] \n",
    "            # has label idx_to_label[j] in the dataset\n",
    "        self.emission_probabilities = emission_count_matrix / emission_count_matrix.sum(axis=1)[:, None]        \n",
    "        # transition_probabilites[i,j] : Probability that idx_to_label[i] follows \n",
    "            # after idx_to_label[j] in the dataset\n",
    "        self.transition_probabilites = transition_count_matrix / transition_count_matrix.sum(axis=1)        \n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(f\"Initializing the model took { end_time - start_time:.4f} seconds\")\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def test_model(train_sentences_debug):\n",
    "    reference_model_dict = pickle.load(  open(\"PartsOfSpeechTaggingModel_test.pkl\", \"rb\"))\n",
    "    debug_model = PartsOfSpeechTaggingModel(train_sentences=train_sentences_debug, min_token_count_required=0)\n",
    "    \n",
    "    assert (reference_model_dict['start_label_probabilities'] == debug_model.start_label_probabilities).all()\n",
    "    assert (reference_model_dict['end_label_probabilities'] == debug_model.end_label_probabilities).all()\n",
    "    assert (reference_model_dict['emission_probabilities'] == debug_model.emission_probabilities).all()\n",
    "    assert (reference_model_dict['transition_probabilites'] == debug_model.transition_probabilites).all()\n",
    "    assert (reference_model_dict['idx_to_label'] == debug_model.idx_to_label)\n",
    "    assert (reference_model_dict['idx_to_token'] == debug_model.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model took 0.0034 seconds\n"
     ]
    }
   ],
   "source": [
    "test_model(train_sentences_debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viterbi Algorithm\n",
    "Now, the viterbi algorithm can be implemented in order to obtain the most-likely sequence of tags for a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def tag_sentence(hmm_model, sentence, verbose=False):\n",
    "    \"\"\"\n",
    "    This method performs part-of-speech tagging on the given sentence using the viterbi algorithm.\n",
    "    :param sentence: str, the sentence to be tagged\n",
    "    :param verbose: bool, if True, the sentence, its tokenization and its predicted tags will be printed\n",
    "    :return: label_sequence_text:  list of str, every element depicts the tag of the respective token in \n",
    "        the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    # replace unknown tokens with placeholder token use it's probabilities as a heuristics    \n",
    "    tokens = sentence.lower().split()\n",
    "    tokens = [token if token in hmm_model.common_tokens else hmm_model.placeholder_token for token in tokens]\n",
    "\n",
    "    # most_probable_ancestor[i,j] :\n",
    "    # If the i-th token of the sentence had label idx_to_label[j]: What's the most likely \n",
    "    # label of the token before?\n",
    "    most_probable_ancestor = np.zeros(shape=(len(tokens), len(hmm_model.label_to_idx)),\n",
    "                                      dtype=np.int)\n",
    "\n",
    "    # frontier_label_probabilities[i] : Probability score of idx_to_label[i] for\n",
    "    # the current token in the viterbi algorithm\n",
    "    frontier_label_probabilities = hmm_model.start_label_probabilities\n",
    "    \n",
    "    #####\n",
    "    # TODO: Your code here    \n",
    "    \n",
    "    # initialize with start label probabilities and first token\n",
    "    frontier_label_probabilities = \n",
    "    \n",
    "    for i, token in enumerate(tokens[1:]):  # other tokens\n",
    "        possible_path_probabilities = \n",
    "        most_probable_ancestor[i] = np.argmax(possible_path_probabilities, axis=1)\n",
    "        most_probable_ancestor_probability =\n",
    "\n",
    "        frontier_label_probabilities = \n",
    "        \n",
    "        # normalize to prevent underflows\n",
    "        frontier_label_probabilities = frontier_label_probabilities \\\n",
    "                                       / frontier_label_probabilities.sum()        \n",
    "\n",
    "    # end label probabilities \n",
    "    frontier_label_probabilities = \n",
    "    \n",
    "    # the label for the last token achieving the highest probability\n",
    "    most_probable_last_label = np.argmax(frontier_label_probabilities)\n",
    "\n",
    "    # list of labels for the tokens, build in reversed order\n",
    "    most_probable_label_sequence = [int(most_probable_last_label)]\n",
    "\n",
    "    for i in range(1, most_probable_ancestor.shape[0]):\n",
    "        last_label = \n",
    "        most_probable_label_sequence.append(int(most_probable_ancestor[-(i + 1), last_label]))\n",
    "    #####\n",
    "\n",
    "    most_probable_label_sequence.reverse()\n",
    "\n",
    "    label_sequence_text = [hmm_model.idx_to_label[idx] for idx in most_probable_label_sequence]\n",
    "\n",
    "    if verbose:\n",
    "        print(sentence)\n",
    "        print(tokens)\n",
    "        print(label_sequence_text)\n",
    "        \n",
    "\n",
    "    return label_sequence_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and evaluate your model\n",
    "### Small dataset for debugging\n",
    "Now, a model for debugging can be instantiated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model took 0.0006 seconds\n"
     ]
    }
   ],
   "source": [
    "debug_model = PartsOfSpeechTaggingModel(train_sentences=train_sentences_debug, min_token_count_required=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the algorithm can be tested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def test_tag_sentence(model):\n",
    "    sentence = \"Mr. Podger had thanked him gravely .\"\n",
    "    label_sequence_text = tag_sentence(model,sentence, verbose = True)\n",
    "    assert label_sequence_text == ['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Podger had thanked him gravely .\n",
      "['mr.', 'podger', 'had', 'thanked', 'him', 'gravely', '.']\n",
      "['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.']\n"
     ]
    }
   ],
   "source": [
    "test_tag_sentence(debug_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The model should be able to overfit on the data and roughly tag the first sentence in the small debug dataset.\n",
    "It does not know how to deal with unknown words though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Podger had thanked him gravely , and now he made use of the advice .\n",
      "['mr.', 'podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.']\n",
      "['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', '.', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'VERB', 'NOUN', '.']\n",
      "The deer is running fast .\n",
      "['the', 'UNKNOWN_PLACEHOLDER_TOKEN', 'is', 'UNKNOWN_PLACEHOLDER_TOKEN', 'UNKNOWN_PLACEHOLDER_TOKEN', '.']\n",
      "['DET', 'ADP', 'VERB', 'PRT', 'CONJ', '.']\n",
      "Google is a company .\n",
      "['UNKNOWN_PLACEHOLDER_TOKEN', 'is', 'a', 'UNKNOWN_PLACEHOLDER_TOKEN', '.']\n",
      "['PRT', 'VERB', 'DET', 'CONJ', '.']\n"
     ]
    }
   ],
   "source": [
    "tags = tag_sentence(debug_model,\"Mr. Podger had thanked him gravely , and now he made use of the advice .\",\n",
    "                                verbose=True)\n",
    "# HINT: correct tags are ['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON',\n",
    "# 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
    "\n",
    "tags = tag_sentence(debug_model,\"The deer is running fast .\", verbose=True)\n",
    "tags = tag_sentence(debug_model,\"Google is a company .\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full dataset\n",
    "After having debugged your implementation, the full model can be created. This might take several minutes. \n",
    "\n",
    "\n",
    "The parameters min_token_count and epsilon might be finetuned for better performance (to pass the final test).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the model took 30.7614 seconds\n"
     ]
    }
   ],
   "source": [
    "pos_model = PartsOfSpeechTaggingModel(train_sentences=train_sentences, min_token_count_required=5, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the previous example can be tagged better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Podger had thanked him gravely , and now he made use of the advice .\n",
      "['mr.', 'podger', 'had', 'thanked', 'him', 'gravely', ',', 'and', 'now', 'he', 'made', 'use', 'of', 'the', 'advice', '.']\n",
      "['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
      "The deer is running fast .\n",
      "['the', 'deer', 'is', 'running', 'fast', '.']\n",
      "['DET', 'NOUN', 'VERB', 'VERB', 'ADV', '.']\n",
      "Google is a company .\n",
      "['UNKNOWN_PLACEHOLDER_TOKEN', 'is', 'a', 'company', '.']\n",
      "['NOUN', 'VERB', 'DET', 'NOUN', '.']\n"
     ]
    }
   ],
   "source": [
    "tags = tag_sentence( pos_model,\"Mr. Podger had thanked him gravely , and now he made use of the advice .\", \n",
    "                          verbose=True)\n",
    "# HINT: correct tags are ['NOUN', 'NOUN', 'VERB', 'VERB', 'PRON', 'ADV', '.', 'CONJ', 'ADV', 'PRON',\n",
    "# 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', '.']\n",
    "\n",
    "tags = tag_sentence(pos_model,\"The deer is running fast .\", verbose=True)\n",
    "tags = tag_sentence(pos_model,\"Google is a company .\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the perfomance\n",
    "Now, the model can be evaluated on the whole dataset, for which the evaluation method below is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(tag_sentence, model, sentences, verbose=False, estimate=False, seed=123):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of the model on the given sentences\n",
    "\n",
    "    :param tag_sentence: callable function (sentence (string) , verbose (bool) -> tagging ([str])), \n",
    "    mapping a sentence to its token's tags\n",
    "    :param sentences: list of list of [(str, str)], A list containing sentences, each sentence is a list of\n",
    "        tuples of the token and the respective tag\n",
    "    :param verbose: bool, whether the prediction should be verbose\n",
    "    :param estimate: bool, whether to just use 100 randomly chosen sentences for evaluation,\n",
    "     recommended if the list of sentences is very large\n",
    "    :param seed: int, seed for the numpy random choice\n",
    "    :return: float, the (estimated) accuracy\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    if estimate:\n",
    "        np.random.seed(seed)\n",
    "        idx = np.random.choice(len(sentences), 100, replace=False)\n",
    "        sentences = [sentences[i] for i in idx]\n",
    "\n",
    "    sentence_strings = [\" \".join([token for token, _ in sentence]) for sentence in sentences]\n",
    "    sentence_labels = [[label for _, label in sentence] for sentence in sentences]\n",
    "\n",
    "    label_count = 0.\n",
    "    correct_label_count = 0.\n",
    "\n",
    "    for sentence_string, true_labels in zip(sentence_strings, sentence_labels):\n",
    "        predicted_labels = tag_sentence(model,sentence_string, verbose=verbose)\n",
    "\n",
    "        label_count += len(true_labels)\n",
    "        correct_label_count += sum(\n",
    "            [predicted_label == true_label for predicted_label, true_label\n",
    "                 in zip(predicted_labels, true_labels)])\n",
    "\n",
    "    acc = correct_label_count / label_count\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\"Evaluating the model on the given data took \", end_time - start_time, \" seconds\")\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate the performance on the dataset. For good practice in Machine Learning, only touch the test set once (in the very end) and aim for a validation accuracy above 0.85 before running the evaluation in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the given data took  0.6678941249847412  seconds\n",
      "estiamated training accuracy:  0.877961555654895\n",
      "Evaluating the model on the given data took  0.6498968601226807  seconds\n",
      "estimated validation accuracy:  0.8847715736040609\n"
     ]
    }
   ],
   "source": [
    "train_acc = evaluate(tag_sentence, pos_model, train_sentences, estimate=True)\n",
    "print(\"estiamated training accuracy: \", train_acc)\n",
    "val_acc = evaluate(tag_sentence, pos_model, val_sentences, estimate=True)\n",
    "print(\"estimated validation accuracy: \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "The final evaluation on the test set should yield an accuracy of >0.8 to pass this exercise. The test should pass in a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def final_test():\n",
    "    \n",
    "    _, _, test_sentences = read_dataset()\n",
    "\n",
    "    test_acc = evaluate(tag_sentence, pos_model, test_sentences, estimate=True, seed = 321)\n",
    "    print(\"estimated test accuracy: \", test_acc)\n",
    "    assert test_acc >= 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "final_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
